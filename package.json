{
    "name": "gpt-llama.cpp",
    "version": "0.1.8",
    "description": "A llama.cpp drop-in replacement for OpenAI's GPT endpoints, allowing GPT-powered apps to run off local llama.cpp models instead of OpenAi.",
    "main": "index.js",
    "type": "module",
    "scripts": {
        "start": "node index.js",
        "dev": "nodemon index.js"
    },
    "author": "kelden",
    "license": "MIT",
    "repository": {
        "type": "git",
        "url": "https://github.com/keldenl/gpt-llama.cpp"
    },
    "dependencies": {
        "child_process": "^1.0.2",
        "cors": "^2.8.5",
        "express": "^4.18.2",
        "fs": "^0.0.1-security",
        "ip": "^1.1.8",
        "nanoid": "^4.0.2",
        "path": "^0.12.7",
        "swagger-jsdoc": "^6.2.8",
        "swagger-ui-express": "^4.6.2"
    },
    "devDependencies": {
        "nodemon": "^2.0.22"
    },
    "bin": {
        "gpt-llama.cpp": "node bin/cli.cjs"
    }
}